# ğŸ§  NLP Message Responder
### _Context-aware AI replies for any message. Full-stack, production-ready._

A minimal but complete AI tool that analyzes any incoming **message** (email, DM, support request, negotiation, complaint, etc.) using NLP, then generates a polished, context-appropriate response.

Built end-to-end with:

- **FastAPI** (backend, OpenAI integration)
- **Vue 3 + Vite** (frontend UI)
- **OpenAI GPT-4o** (NLP + response generation)
- **Render** (backend hosting)
- **Vercel** (frontend hosting)

This project demonstrates real full-stack execution: backend design, frontend UX, AI prompting, deployment, and product structure.

---

## ğŸš€ Features

### ğŸ” NLP-Driven Message Analysis  
The backend uses a structured OpenAI prompt to:
- Detect intent  
- Extract sentiment  
- Parse contextual hints  
- Understand subtext  
- Produce tailored, human-sounding replies  

### âœï¸ Generate Context-Aware Responses  
Paste any message â†’ Receive a clean, ready-to-send reply.

### ğŸ” Local OpenAI Key Storage  
No backend storage.  
User API keys are stored **securely in localStorage**â€”never transmitted or logged by the backend.

### âš™ï¸ Dedicated Settings Screen  
Includes:
- API key input  
- Key validation (verified with OpenAI before saving)  
- â€œShow/hide keyâ€ eye toggle  

### ğŸ’¡ Clean, Fast Frontend  
Vue 3 + Vite with:
- Router navigation  
- Simple two-page UX  
- Minimal, focused UI  
- Default NLP mode (no tone selection needed)  

### ğŸŒ Production Deployment  
- **Backend â†’ Render**  
- **Frontend â†’ Vercel**  
- CORS-ready  
- Zero backend secrets from the user

---

## ğŸ§© Tech Stack
### **Frontend**
- Vue 3  
- Vite  
- Vue Router  
- LocalStorage (API key)  

### **Backend**
- FastAPI  
- Pydantic  
- OpenAI responses API  
- Render deployment  

### **AI**
- OpenAI GPT-4o  
- Prompt templates stored on backend  
- Structured NLP-driven reply engine  

### **Deployment**
- Vercel (frontend)  
- Render (backend)

---

## ğŸ“¦ Project Structure
```txt
NLP-message-responder/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ main.py
â”‚ â”‚ â”œâ”€â”€ schemas.py
â”‚ â”‚ â”œâ”€â”€ call_gpt.py
â”‚ â”‚ â”œâ”€â”€ router_email.py
â”‚ â”‚ â”œâ”€â”€ constants.py
â”‚ â”‚ â””â”€â”€ config.py
â”‚ â”œâ”€â”€ pyproject.toml
â”‚ â””â”€â”€ requirements.txt
â””â”€â”€ frontend/
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ App.vue # root router shell
â”‚ â”œâ”€â”€ Home.vue # main responder page
â”‚ â”œâ”€â”€ Settings.vue # API key management
â”‚ â”œâ”€â”€ router.js
â”‚ â””â”€â”€ composables/
â”‚ â”‚ â””â”€â”€ useSettings.js
â”œâ”€â”€ main.css
â””â”€â”€ vite.config.js
```

âš™ï¸ Backend Setup
```bash
cd backend
uv sync
uv run uvicorn app.main:app --reload
```

Run locally:
```bash
uvicorn app.main:app --reload
```

API available at:
```
http://localhost:10000/api/response
```

ğŸ¨ Frontend Setup
```bash
cd frontend
npm install
npm run dev
```

Set backend URL inside your frontend .env:
VITE_API_URL=http://localhost:10000/api

ğŸŒ Deployment
Backend â†’ Render
Deploy /backend

Render will expose your public API endpoint
Frontend â†’ Vercel
Deploy /frontend
Add API_URL in Vercel environment variables

ğŸ§  How It Works
User pastes an email
 -> Selects the tone
 -> Frontend sends data to /api/respond
 -> FastAPI composes an NLP prompt
 -> OpenAI generates the email reply
 -> Response returned instantly

ğŸ“œ License
MIT 
